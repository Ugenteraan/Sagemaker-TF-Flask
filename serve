#!/usr/bin/env python3.5
import tensorflow as tf 
import numpy as np 
import cv2 
import os
from flask import Flask, request, jsonify
import base64
import boto3
import settings

app=Flask(__name__) #initialize Flask.

#reset tf graph and initialize a session.
tf.reset_default_graph()
sess = tf.InteractiveSession()


s3 = boto3.resource('s3', region_name='ap-southeast-1', aws_access_key_id=settings.ACCESS_KEY, aws_secret_access_key=settings.SECRET_KEY)
bucket = s3.Bucket('test-ml-access')
object_meta = bucket.Object('mnist.ckpt.meta')
object_ckpt_1 = bucket.Object('mnist.ckpt.index')
object_ckpt_2 = bucket.Object('checkpoint')
object_ckpt_3 = bucket.Object('mnist.ckpt.data-00000-of-00001')


with open('./temp.ckpt.meta', 'wb') as f:
	object_meta.download_fileobj(f)

with open('./temp.ckpt.index', 'wb') as f:
	object_ckpt_1.download_fileobj(f)

with open('./checkpoint', 'wb') as f:
	object_ckpt_2.download_fileobj(f)

with open('./temp.ckpt.data-00000-of-00001', 'wb') as f:
	object_ckpt_3.download_fileobj(f)



#load the metadata of the model.
saver = tf.train.import_meta_graph('./temp.ckpt.meta')


#restore the model's parameters from the checkpoint file.
try:
	saver.restore(sess, 'temp.ckpt')
	print("Model has been loaded!")

except Exception as e:
	print(e) #print the exception.
	print("Model has not been loaded!")


graph = tf.get_default_graph()


#get the variables that were added earlier during training from the collection.
logits = graph.get_collection('logits')[0]
x = graph.get_collection('x')[0]



#sagemaker's endpoint
@app.route('/invocations', methods=['POST'])
def invoke():
	'''
	Input : Base64 encoded image.
	Output: Inference of the model.
	'''

	imgdata = base64.b64decode(request.data) #decode the base64 encoded image.

	filename = 'inference_image.jpg' 

	#write the image to the current working directory under the name of 'filename'.
	with open(filename, 'wb') as f:
		f.write(imgdata) 

	image = cv2.imread(filename, 0) #read the image in grayscale mode.
 
	resized = cv2.resize(image, (784,1)) #the trained MNIST model only accepts a 784-vector. Hence, resize is a must.

	result = sess.run(logits, feed_dict={x:resized}) #infer.

	digit = np.argmax(result) #get the highest probability class.

	#returns a json with the prediction class.
	return jsonify({
		"Result" : str(digit)
		})



#sagemaker requires this endpoint for healthcheck purpose.
@app.route('/ping', methods=['GET'])
def healthcheck():
	'''
	Returns a json response with status 200.
	'''
	response = {}
	response['status'] = 200
	response['message'] = 'OK'

	return jsonify(response)



#sagemaker requires the webserver to be running on port 8080.
if __name__ =='__main__':
	app.run(host='0.0.0.0',debug=False, port=8080)